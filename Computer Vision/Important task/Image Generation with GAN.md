GAN 在图像生成上取得了巨大的成功，这无疑取决于 GAN 在博弈下不断提高建模能力，最终实现以假乱真的图像生成。

GAN 自 2014 年诞生至今也有 4 个多年头了，大量围绕 GAN 展开的文章被发表在各大期刊和会议，以改进和分析 GAN 的数学研究、提高 GAN 的生成质量研究、GAN 在图像生成上的应用（指定图像合成、文本到图像，图像到图像、视频）以及 GAN 在 NLP 和其它领域的应用。图像生成是研究最多的，并且该领域的研究已经证明了在图像合成中使用 GAN 的巨大潜力。

本文围绕 ***An Introduction to Image Synthesis with Generative Adversarial Nets*** 一文对 GAN 在图像生成应用做个综述。

# 论文引入

著名的物理学家 Richard Feynman 说过：*“What I cannot create, I do not understand”（对于我创造不出的事物，我是无法理解它的）*。我们现阶段接触到的 AI 产品，都是在尝试去看懂人类可以看懂的，例如对 ImageNet 的图像分类、AlphaGo、智能对话机器人等。

然而，我们仍然不能断定这些算法具有真正的“智能”，因为知道如何做某事并不一定意味着理解某些东西，而且真正智能的机器人理解其任务是至关重要的。

如果机器可以去 create，这也就可以说明机器对它的输入数据已经可以自主的建模，这是否可以说明机器在朝着更加“智慧”迈进了一步。这种 create 在机器学习的领域下，目前最为可行的方法是生成模型。通过学习的生成模型，机器甚至可以绘制不在训练集中但遵循相同分布的样本。

在生成模型中比较有影响力的有 ***VAE*** 、***PixelCNN*** 、***Glow*** 、***GAN*** 。其中在 2014 年提出的 GAN 可谓是生成模型中最受欢迎的，即使不能说 GAN 是一骑绝尘但也可谓是鹤立鸡群。

GAN 由两个神经网络组成，一个生成器和一个判别器组成，其中生成器试图产生欺骗判别器的真实样本，而判别器试图区分真实样本和生成样本。这种对抗博弈下使得生成器和判别器不断提高性能，在达到纳什平衡后生成器可以实现以假乱真的输出。

但是这种纳什平衡只存在于理论中，实际 GAN 的训练伴随着一些问题的限制。一个是 GAN 训练不稳定性另一个是模式崩溃。

GAN 存在的问题并没有限制 GAN 的发展，不断改进 GAN 的文章层出不穷，在这几年的铺垫下 GAN 已经发展得蛮成熟的。从这几年关于 GAN 的高质量文章可以看出，18 年以后的文章更多关注的是 GAN 在各个领域的应用，而之前的文章则是集中在 GAN 存在问题的改进。

GAN 在图像生成应用最为突出，当然在计算机视觉中还有许多其他应用，如图像绘画，图像标注，物体检测和语义分割。在自然语言处理中应用 GAN 的研究也是一种增长趋势，如文本建模，对话生成，问答和机器翻译。然而，在 NLP 任务中训练 GAN 更加困难并且需要更多技术，这也使其成为具有挑战性但有趣的研究领域。

***An Introduction to Image Synthesis with Generative Adversarial Nets\*** 一文是**概述 GAN 图像生成中使用的方法，并指出已有方法的优缺点**。本文则是对这篇论文进行个人的理解和翻译，并对其中的一些方法结合个人实际应用经验进行分析。

# GAN的基础

接触过 GAN 的学者如果对 GAN 的结构已经很熟悉，这一部分可以自行跳过。我们看一下 **GAN 的基础结构：**

![img](https://pic2.zhimg.com/80/v2-53383fc5a1344b890850cab4abfa588a_1440w.jpg)

GAN 可以将任意的分布作为输入，这里的 Z 就是输入，在实验中我们多取Z∼N(0,1)，也多取 [−1,1] 的均匀分布作为输入。生成器 G 的参数为 θ，输入 Z 在生成器下得到 G(z;θ)，输出可以被视为从分布中抽取的样本 G(z;θ)∼Pg。

对于训练样本 x 的数据分布为 Pdata，生成模型 G 的训练目标是使 Pg 近似Pdata。判别器 D 便是为了区分生成样本和真实样本的真假，训练发生器和判别器通过最小 - 最大游戏，其中**发生器 G 试图产生逼真的数据以欺骗判别器，而判别器 D 试图区分真实数据和合成数据**。这种博弈可公式化为：
$$
\min_G \max_D V(D,G) = E_{x \sim p_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(Z)))]
$$
最初的 GAN 使用完全连接的层作为其构建块。后来，***DCGAN*** 提出使用卷积神经网络实现了更好的性能，从那以后卷积层成为许多 GAN 模型的核心组件。

然而，当判别器训练得比发生器好得多时，D 可以有信心地从 G 中拒绝来自 G 的样本，因此损失项 log(1−D(G(z))) 饱和并且 G 无法从中学到任何东西。

为了防止这种情况，可以训练 G 来最大化 logD(G(z))，而不是训练 G 来最小化 log(1−D(G(z)))。虽然 G 的改变后的损失函数给出了与原始梯度不同的梯度，但它仍然提供相同的梯度方向并且不会饱和。

# 条件GAN

在原始 GAN 中，无法控制要生成的内容，因为输出仅依赖于随机噪声。我们可以将条件输入 c 添加到随机噪声 Z，以便生成的图像由 G(c,z) 定义。这就是 ***CGAN*** ，通常条件输入矢量 c 与噪声矢量 z 直接连接即可，并且将得到的矢量原样作为发生器的输入，就像它在原始 GAN 中一样。条件 c 可以是图像的类，对象的属性或嵌入想要生成的图像的文本描述，甚至是图片。

# 辅助分类器GAN (ACGAN)

为了提供更多的辅助信息并允许半监督学习，可以向判别器添加额外的辅助分类器，以便在原始任务以及附加任务上优化模型。这种方法的体系结构如下图所示，其中 C 是辅助分类器。

添加辅助分类器允许我们使用预先训练的模型（例如，在 ImageNet 上训练的图像分类器），并且在 ***ACGAN\*** 中的实验证明这种方法可以帮助生成更清晰的图像以及减轻模式崩溃问题。使用辅助分类器还可以应用在文本到图像合成和图像到图像的转换。

![img](https://pic2.zhimg.com/80/v2-9ee70fc2f3f5189b194693a6a0bdf166_1440w.jpg)

# GAN与Encoder的结合

尽管 GAN 可以将噪声向量 z 转换为合成数据样本 G(z)，但它不允许逆变换。如果将噪声分布视为数据样本的潜在特征空间，则 GAN 缺乏将数据样本 x 映射到潜在特征 z 的能力。

为了允许这样的映射，两个并发的工作 ***BiGAN*** [8] 和 ***ALI*** [9] 在原始 GAN 中添加编码器 E，如下图所示。

![img](https://pic2.zhimg.com/80/v2-55f52467212331195723807e1403c4ca_1440w.jpg)

令 Ωx 为数据空间，Ωz 为潜在特征空间，编码器 E 将 x∈Ωx 作为输入，并产生特征向量 E(x)∈Ωz 作为输出。修正判别器 D 以将数据样本和特征向量都作为输入来计算 P(Y|x,z)，其中 Y=1 表示样本是真实的而 Y=0 表示数据由 G 生成。用数学公式表示为：
$$
\min_{G,E} \max_D V(G,E,D) = E_{x \sim p_{data}(x)}logD(x,E(x))+E_{z\sim p_z(z)}log(1-D(G(Z),z))
$$
**GAN与VAE的结合**

VAE 生成的图像是模糊的，但是 VAE 生成并没有像 GAN 的模式崩溃的问题，***VAE-GAN***  的初衷是结合两者的优点形成更加鲁棒的生成模型。模型结构如下：

![img](https://picb.zhimg.com/80/v2-6d95f1729d2275bf19a2a24927e58e39_1440w.jpg)

但是实际训练过程中，VAE 和 GAN 的结合训练过程也是很难把握的。

# 处理模式崩溃问题

虽然 GAN 在图像生成方面非常有效，但它的训练过程非常不稳定，需要很多技巧才能获得良好的结果。**GAN 不仅在训练中不稳定，还存在模式崩溃问题**。判别器不需要考虑生成样品的种类，而只关注于确定每个样品是否真实，这使得生成器只需要生成少数高质量的图像就足以愚弄判别者。

例如在 MNIST 数据集包含从 0 到 9 的数字图像，但在极端情况下，生成器只需要学会完美地生成十个数字中的一个以完全欺骗判别器，然后生成器停止尝试生成其他九位数，缺少其他九位数是类间模式崩溃的一个例子。类内模式崩溃的一个例子是，每个数字有很多写作风格，但是生成器只学习为每个数字生成一个完美的样本，以成功地欺骗鉴别器。

目前已经提出了许多方法来解决模型崩溃问题。一种技术被称为**小批量（miniBatch）特征**，其思想是使判别器比较真实样本的小批量以及小批量生成的样本。通过这种方式，判别器可以通过测量样本在潜在空间中的距离来学习判断生成的样本是否与其他一些生成的样本过于相似。尽管这种方法运行良好，但性能在很大程度上取决于距离计算中使用的特征。

***MRGAN***  建议添加一个编码器，将数据空间中的样本转换回潜在空间，如 BiGAN 它的编码器和生成器的组合充当自动编码器，重建损失被添加到对抗性损失中以充当模式正则化器。同时，还训练判别器以区分重构样本，其用作另一模式正则化器。

***WGAN*** 使用 Wasserstein 距离来测量真实数据分布与学习分布之间的相似性，而不是像原始 GAN 那样使用 Jensen-Shannon 散度。虽然它在理论上避免了模式崩溃，但模型收敛的时间比以前的 GAN 要长。

为了缓解这个问题，***WGAN-GP*** 建议使用梯度惩罚，而不是 WGAN 中的权重削减。WGAN-GP 通常可以产生良好的图像并极大地避免模式崩溃，并且很容易将此培训框架应用于其他 GAN 模型。

***SAGAN*** 将谱归一化的思想用在判别器，限制判别器的能力。

# GAN在图像生成方法

GAN 在图像生成中的主要方法为**直接方法**，**迭代方法**和**分层方法**，这三种方法可由下图展示











































