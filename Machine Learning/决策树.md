# 决策树模型与学习

决策树学习通常包括3个步骤：特征选择，决策树生成，剪枝。

决策树的内部结点表示一个特征或属性，叶结点表示一个类。

## 决策树模型

分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点分为内部结点和叶结点。

## 决策树与if-then规则

**If-then**：决策树路径或其对应的if-then规则集合具有一个重要的性质，互斥并且完备，也就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或者一条规则覆盖。

## 决策树与条件概率分布

**概率分布**：决策树将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布。决策树的一条路径对应于划分中的一个单元，决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成，即$𝑃(𝑌|𝑋)$，叶结点（单元）上的条件概率往往偏向某一类。

## 决策树学习

假设给定训练数据集：
$$
D = {(x_1,y_1),(x_2,y_2),...(x_N,y_N)}
$$
其中，$x_j=(x_j^{(1)},(x_j^{(2)},...,(x_j^{(n)})^T$为输入实例，$n$为特征个数，$y_i \in{1,2,...,K}$为类标记

![DecisonTree](../img/ML/DecisonTree.jpg)

# 特征选择

## 特征选择问题

## 特征选择问题

特征选择在于选取对训练数据具有分类能力的特征。

如果利用一个特征进行分类的结果与随机分类的结果没有很大区别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的影响不大。

## 信息增益

设$X$是一个取有限个值的离散随机变量，其概率分布为：
$$
P(X=x_i)=p_i,i=1,2,...,n
$$
则随机变量$X$的熵定义为：
$$
H(X)=-\sum_{i=1}^np_ilogp_i
$$
**熵（entropy）**：熵度量的是随机变量的不确定性。熵越大，不确定性越大。

设有随机变量(X,Y)，其联合概率分布为：
$$
P(X=x_i,Y=y_j)=p_{ij},i=1,2,...,n;j=1,2,...,m
$$
条件熵$H(Y|X)$：
$$
H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i)
$$
同时有：
$$
p_i=P(X=x_i),i=1,2,...,n
$$
**信息增益**
$$
g(D,A)=H(D)-H(D|A)
$$

## 信息增益比

$$
g_R(D,A)=\frac{g(D,A)}{H(D)}
$$

# 决策树的生成

## ID3算法

ID3的核心是在各个结点上应用“信息增益”准则选择特征。

从根结点出发，选择信息增益最大的特征作为结点特征，由该特征的不同取值建立子结点，对子结点递归调用以上方法。直到所有特征的信息增益均很小（设一个阈值$\epsilon$）或没有特征可选为止。

















































