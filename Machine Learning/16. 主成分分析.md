主成分分析（Principal components analysis，以下简称PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。一般我们提到降维最容易想到的算法就是PCA。

# PCA的思想

PCA顾名思义，就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是$n$维的，共有m个数据$(𝑥_{(1)},𝑥_{(2)},...,𝑥_{(𝑚)})$。我们希望将这$m$个数据的维度从$n$维降到$n'$维，希望这$m$个$n'$维的数据集尽可能的代表原始数据集。我们知道数据从$n$维降到$n'$维肯定会有损失，但是我们希望损失尽可能的小。

- 样本点到这个直线的距离足够近
- 样本点在这个直线上的投影能尽可能的分开。

# PCA的推导:基于最小投影距离

假设m个n维数据$(x^{(1)}, x^{(2)},...,x^{(m)})$都已经进行了中心化，即$\sum\limits_{i=1}^{m}x^{(i)}=0$。经过投影变换后得到的新坐标系为$\{w_1,w_2,...,w_n\}$其中$w$是标准正交基，即$||w||_2=1, w_i^Tw_j=0$

如果我们将数据从n维降到n'维，即丢弃新坐标系中的部分坐标，则新的坐标系为$\{w_1,w_2,...,w_{n'}\}$，样本点$x^{(i)}$在n'维坐标系中的投影为：$z^{(i)} = (z_1^{(i)}, z_2^{(i)},...,z_{n'}^{(i)})^T$，$z_j^{(i)} = w_j^Tx^{(i)}$是$x^{(i)}$在低维坐标系里第j维的坐标。

如果我们用$z^{(i)}$，来恢复原始数据$x^{(i)}$，则得到的恢复数据$\overline{x}^{(i)} = \sum\limits_{j=1}^{n'}z_j^{(i)}w_j = Wz^{(i)}$其中，W为标准正交基组成的矩阵。

**现在我们考虑整个样本集，我们希望所有的样本到这个超平面的距离足够近，即最小化下式：**
$$
\sum\limits_{i=1}^{m}||\overline{x}^{(i)} - x^{(i)}||_2^2
$$
将这个式子进行整理，可以得到:
$$
\begin{align} \sum\limits_{i=1}^{m}||\overline{x}^{(i)} - x^{(i)}||_2^2 & = \sum\limits_{i=1}^{m}|| Wz^{(i)} - x^{(i)}||_2^2 \\& = \sum\limits_{i=1}^{m}(Wz^{(i)})^T(Wz^{(i)}) - 2\sum\limits_{i=1}^{m}(Wz^{(i)})^Tx^{(i)} + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& = \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} - 2\sum\limits_{i=1}^{m}z^{(i)T}W^Tx^{(i)} +\sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& = \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} - 2\sum\limits_{i=1}^{m}z^{(i)T}z^{(i)}+\sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \\& = - \sum\limits_{i=1}^{m}z^{(i)T}z^{(i)} + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \\& =   -tr( W^T（\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T})W)  + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)} \\& =  -tr( W^TXX^TW)  + \sum\limits_{i=1}^{m} x^{(i)T}x^{(i)}  \end{align}
$$
注意到$\sum\limits_{i=1}^{m}x^{(i)}x^{(i)T}$

