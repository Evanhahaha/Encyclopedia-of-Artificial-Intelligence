# 提升方法AdaBoost算法

## 提升方法的基本思路

对于一个复杂任务来说， 将多个专 家的判断进行适当的综合所得出的判断， 要比其中任何一个专家单独 的判断好。 实际上， 就是“三个臭皮匠顶个诸葛亮”的道理  

 在概率近似正确（probably approximately correct， PAC） 学习的框架中， 如果存在一个多项式的学习算法能够学习它， 并 且正确率很高， 那么就称这个概念是强可学习的； 

如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学的。

**提升方法的核心问题**

对提升方法来说， 有两个问题需要回答： 一是在每一轮如 何改变训练数据的权值或概率分布； 二是如何将弱分类器组合成一个强分类器。

## Adaboost

**Idea**

- AdaBoost的做法是提高那些被前一轮弱分类器错误分类样本的权值， 而降低那些被正确分类样本的权值。 这样一来， 那些没有得到正确分类的数据， 由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。 
- 即弱分类器的组合， AdaBoost采取加权多数表决的方法。 具体地， 加大分类误差率小的弱分类器的权值， 使其在表决中起较大的作用， 减小分类误差率大的弱分类器的权值， 使其在表决中起较小的作用。AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一 种算法里。  

**Step**

输入：
$$
T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}，x_i\in\mathcal{X}\sube R^n，y_i \in\mathcal{Y}={-1,+1}
$$
初始化权值分布：
$$
D_1=(w_{11},...w_{1i},...,w_{1N})，w_{1i}=\frac{1}{N}，i=1,2,...,N
$$
基本分类器：
$$
G_m(x): \mathcal{X}\to\{-1,+1\}
$$
分类误差率：
$$
e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)
$$
系数：
$$
\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
$$
更新权值分布：
$$
D_{m+1}=(w_{\{m+1,1\}},...,w_{\{m+1,i\}},...,w_{\{m+1,N\}})
$$
以及：
$$
w_{\{m+1,i\}} = \frac{w_{\{m,i\}}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,...,N
$$
这里的$Z_m$是规范化因子：
$$
Z_m = \sum_{i=1}^Nw_{\{m,i\}}exp(-\alpha_my_iG_m(x))
$$
最终分类器：
$$

$$










































