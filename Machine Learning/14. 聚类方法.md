# 聚类的基本概念

**聚类的核心是相似度或距离。** 

## 相似度或距离

聚类的对象是观测数据或样本集合。假设有n个样本，每个样本有m个属性的特征向量组成。样本集合表示为:
$$
X = [x_{ij}]_{m \times n}=\left[
 \begin{matrix}
   x_{11} & x_{12} & ...  &x_{1n}\\
   x_{21} & x_{22} & ... & x_{2n} \\
   ... & ... & ...&...\\
   x_{m1} &x_{m2}&...&x_{mn}
  \end{matrix}
  \right] 
$$


### 闵可夫斯基距离

在聚类中，可以将样本集合想象成向量空间中的点，可以以空间的距离表示样本之间的相似度：
$$
d_{ij}=(\sum_{k=1}^m|x_{ki}-x_{kj}|^p)^{\frac{1}{P}}
$$
当$p=2$的时候为欧式距离

当$p=1$的时候为曼哈顿距离

当$p=∞$时称为切比雪夫距离，取各个坐标差点最大值
$$
d_{ij}=\max_k|x_{ki}-x_{kj}|
$$

### 马哈拉诺比斯距离

马哈拉诺比斯距离，简称马氏距离，是另一种常用的相似度。考虑各个分量(特征)之间的相关性并与各个分量的尺度无关。马哈拉诺比斯距离越大相似度越小，距离越小相似度越大。
$$
d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^{\frac{1}{2}}
$$

### 相关系数

样本$x_i$与样本$x_j$之间的相关系数定义为：
$$
r_{ij}=\frac{\sum_{k=1}^M(x_{ki}-\overline{x_i})(x_{kj}-\overline{x_j})}{[\sum_{k=1}^M(x_{ki}-\overline{x_i})^2\sum_{k=1}^M(x_{kj}-\overline{x_j})^2]^\frac{1}{2}}
$$


其中：
$$
\overline{x_i}=\frac{1}{m}\sum_{k=1}^mx_{ki} \\
\overline{x_j}=\frac{1}{m}\sum_{k=1}^mx_{kj} \\
$$

### 夹角余弦

$$
s_{ij}=\frac{\sum_{k=1}^mx_{ki}x_{kj}}{[\sum_{k=1}^mx_{ki}^2x_{kj}^2]^{\frac{1}{2}}}
$$

## 类或簇

通过聚类得到的类或簇，本质是样本的子集。如果一个聚类方法假定一个样本只能属于一个类，或类的交集为空集，那么该方法称为硬聚类方法。否则，如果一个样本可以属于多个类，或类的交集不为空集，那么该方法称为软聚类方法。

用$G$表示类或簇，用$x_i,x_j$表示类中的样本，用$n_G$表示$G$中样本的个数，用$d_{ij}$表示样本$x_i$和$x_j$之间的距离

设$T$为给定的正数，若集合$G$中任意两个样本$x_i$，$x_j$
$$
d_{ij}\leq T
$$
设$T$为给定的正数，若集合$G$的任意样本$x_i$ ，一定存在$G$中的另一个样本$x_j$：
$$
d_{ij}\leq T
$$




















