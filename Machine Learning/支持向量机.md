# 线性可分支持向量机与硬间隔最大化

- Linear support vector machine liearly separable case
- linear support vector machine
- Non-linear support vector machine



## 线性可分支持向量机

给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到分离超平面：
$$
w^* \cdot x + b ^* = 0
$$
以及相应的分类决策函数：
$$
f(x)=sign(w^* \cdot x + b ^*)
$$

## 函数间隔和几何间隔

**函数间隔：**
$$
\widehat{\gamma^*_i}=y_i(w \cdot x_i + b)
$$
即得到：
$$
\widehat{\gamma}= \min_{1,..,N}\widehat{r_i}
$$
进行规范化得到几何间隔：
$$
\gamma_i=y_i(\frac{w}{||w||} \cdot x_i + b||w||)
$$
即有：
$$
\gamma= \min_{i=1,...N}\gamma_i
$$
建议二者的关系：
$$
\gamma ||w|| = \widehat{\gamma}
$$

## 间隔最大化

**最大间隔分离超平面**
$$
\max_{w,b} \gamma \\
s.t. y_i(\frac{w}{||w||} \cdot x_i+\frac{b}{||w||}) \geq \gamma,i=1,2,...,N
$$
等价于：
$$
\max_{w,b} \frac{\widehat{\gamma}}{||w||} \\
s.t. y_i({w} \cdot x_i+{b}) \geq \widehat{\gamma},i=1,2,...,N
$$
从某种程度上，可以得到如下的最优化问题，假定$\widehat{\gamma}=1$：
$$
\min_{w,b} \frac{1}{2}||w||^2 \\
s.t. y_i(w \cdot x_i+b)-1 \geq 0 i=1,2,...,N
$$
转化为凸二次规划问题即可

**线性可分训练数据集的最大间隔分离超平面是存在且唯一的**

**支持向量和间隔边界**

支持向量是使约束条件式等号成立的点：
$$
y_i = (w \cdot x_i + b)=1
$$
间隔依赖于分离超平面的法向量$w$，等于\frac{2}{w}

## 学习的对偶算法

对偶形式的目的是降低运算量，但是并不是在任何情况下都能降低运算量，而是在特征空间的维度很高时才起到作用。

首先定义拉格朗日函数：
$$
L(w,b,\alpha) = \frac{1}{2}||w||^2+\sum_{i=1}^N\alpha_i(1-y_i(w \cdot x_i+b))
$$
我们首先要明白，如果要满足条件第二部分的值会要小于0，而我们知道系数$\alpha_i$是大于0的，那么这个式子不存在最小值，而对于第一部分又要尽可能的小，所以得到
$$
\max_a\min_{w,b}L(w,b,\alpha)
$$
其次：
$$
\bigtriangledown_w
$$






































